定理证明
准备工作：
注释：
 
定义：
	传输过程
 
根据参数的进出我们可以得到如下几个定义：
H^((l))=F^((l) ) (H^((l-1) );θ^((l)))
这里¬H^((l))的维度为N×D^((l))，N是batch size，D^((l))是特征的数量

在反向传播中，我们可以把反向传播需要计算的梯度定义为：
∇_(H^((l-1)) )  ,∇_(θ^((l)) )  = G^((l) ) (∇_(H^((l) ) ),C(H^((l-1) ),θ^((l))))
这里C(~)是反向传播中需要被储存的信息。l层的G^((l))函数使用上层输出的∇_(H^((l) ) )和C(~)计算得到l层的梯度。初始的，假定∇_(H^((l)) )  和 ∇_(θ^((l)) )是全精度的（full precision）梯度。并且考虑一个比较简单的情形——线性层：H^((l) )=H^((l-1) ) θ^((l))，我们可以通过chain rule和梯度方程的相关计算公式得到相应的梯度：
∇_(H^((l-1)) )=∇_(H^((l)) ) θ^((l)^T ),∇_(θ^((l)) )=H^((l-1)^T ) ∇_(H^((l)) )
这里证明不做过多赘述，写于appendix中
并且在这一线性情况下，C(H^((l-1) ),θ^((l) ) )=(H^((l-1) ),θ^((l)))

	ActNN准备工作：
定义，这里我们在相关的标识符上加一个hat表示这是关于ActNN的。
在反向传播中用到储存的信息不再是C(H^((l-1) ),θ^((l)))，而是C ̂(H^((l-1) ),θ^((l)))
并且，梯度计算为∇ ̂_(H^((l-1)) )  ,∇ ̂_(θ^((l)) )  = G^((l) ) (∇ ̂_(H^((l) ) ),C ̂(H^((l-1) ),θ^((l))))，注意到在最后一层向前传播（L层）并转为向后传播时候并没有受到存储的激活压缩信息而非全精度信息的影响，因此此时∇_(H^((L)) )=∇ ̂_(H^((L)) )。

进一步的，如果我们把特征矩阵梯度进行细化，通过chain rule对每一个元素都进行计算可以得到，∑对所有的kl偏导求和没关系，不相关没用到的都是0：
∇_(H_ij^((l-1) ) )=〖∑_kl▒(∂H_kl^((l)))/(∂H_ij^((l-1)) ) ∇〗_(H_kl^((l)) ),∇_(θ_i^((l) ) )=〖∑_kl▒(∂H_kl^((l)))/(∂θ_i^((l-1)) ) ∇〗_(H_kl^((l)) )
每次迭代都会产生一个系数θ，记作θ_t={θ^((l) ) }_(l=1)^L，作为参数的扁平向量，相应的，
∇_(θ_t )  ={∇_(θ_t^((l)) )  }_(l=1)^L,∇ ̂_(θ_t )  ={∇ ̂_(θ_t^((l)) )  }_(l=1)^L分别对应全精度和激活压缩的梯度。
L_D (θ)是整个数据集上的小样本批次损失（batch loss），同时∇_θ,∇ ̂_θ都是批次梯度∇ ̂_θ L_D (θ)的随机估计量，因为∇_θ的随机性进来自于batch size，是小批量的采样，在这里我们假设这是无偏的，所以E[ ∇_θ ]= ∇_θ L_D (θ)。而∇ ̂_θ的随机性还来自于C(~)的随机量化，如果这也是无偏的，那么第一步就完成了（第二步是验证AC方差和全精度方差是否相近，进而判断是ActNN是否可以替代）。

	T1定理描述（无偏梯度）：C ̂存在随机量化策略使得E[ ∇ ̂_θ ]= ∇_θ L_D (θ)
证明：
先有引理：Lemma1：如果E[ ∇ ̂_(H^((l)) ) ]=E[∇_(H^((l) ) ) ]，那么存在C ̂^((l))，使得E[ ∇ ̂_(H^((l-1)) ) ]=E[∇_(H^((l-1) ) ) ]并且E[ ∇ ̂_(θ^((l)) ) ]=E[∇_(θ^((l) ) ) ]
证：先前提到由链式法则我们有
∇_(H_ij^((l-1) ) )=〖∑_kl▒(∂H_kl^((l)))/(∂H_ij^((l-1)) ) ∇〗_(H_kl^((l)) ),∇_(θ_i^((l) ) )=〖∑_kl▒(∂H_kl^((l)))/(∂θ_i^((l-1)) ) ∇〗_(H_kl^((l)) )
因此，我们有G^((l) ) (∇_(H^((l) ) ),C(H^((l-1) ),θ^((l) ) ))={〖∑_kl▒(∂H_kl^((l) ))/(∂H_ij^((l-1) ) ) ∇〗_(H_kl^((l) ) ) }_ij,{〖∑_kl▒(∂H_kl^((l) ))/(∂θ_i^((l-1) ) ) ∇〗_(H_kl^((l) ) ) }_i ，C(H^((l-1) ),θ^((l) ) )={(∂H_kl^((l) ))/(∂H_ij^((l-1) ) ),(∂H_kl^((l) ))/(∂θ_i^((l) ) )}_ijkl
令C ̂(H^((l-1) ),θ^((l) ) )=Q(C(H^((l-1) ),θ^((l) ) ))，Q(∙)是一个无偏算子，s.t.对于任意x都有E[Q(x)]=x
可以得到
 
然后我们回到定理1中，因为先前说到∇_(H^((L)) )=∇ ̂_(H^((L)) )，所以〖E[∇〗_(H^((L)) )]=〖E[∇ ̂〗_(H^((L)) )]，然后我们根据引理1和数学归纳法，可以得到对任意的l∈{1,…,L}，均有E[ ∇ ̂_(θ^((l)) ) ]=E[∇_(θ^((l) ) ) ]，所以E[ ∇ ̂_θ ]=E[∇_θ ]
又因为根据假设我们有E[ ∇_θ ]= ∇_θ L_D (θ)，所以E[ ∇ ̂_θ ]= ∇_θ L_D (θ)
在T2开始前，我们假定SGD迭代形如θ_(t+1)←θ_t-α∇ ̂_(θ_t )  (t≥1)，并有如下三个条件的存在：
A1：损失方程L_D (θ)连续可微，且其梯度是β利普西茨连续的，即存在β使得
||  ∇L_D (θ_(t_1 ) )  -∇L_D (θ_(t_2 ) )||≤β|| θ_(t_1 )- θ_(t_2 ) ||
A2：L_D (θ)有下界，定为L_inf
A3有界性：存在σ^2>0，使得对任意θ，Var[∇ ̂_θ ]≤σ^2，向量的方差为Var [x]≔E〖||x||〗^2- 〖||E[x]||〗^2
	T2定理描述（收敛性）：如果满足A1-A3，并且0<α≤1/β，如果t从{1,…,T}中取得（T为最大迭代次数）且取任意值的概率相同，那么有：
E〖||  ∇L_D (θ_t )  ||〗^2≤2(L(θ_1 )-L_inf )/αT+αβσ^2
可以直观的从中看出，随着最大迭代次数T的增加，+号前的部分会趋于0，因此这个梯度会收敛于一个值，由方差控制。
证明：
在论文Optimization methods for large-scale machine learning中，有一个结论
L(θ_(t+1) )-L(θ_t )≤∇L(θ_t )^T (θ_(t+1)-θ_t )+1/2 β|| θ_(t+1)-θ_t  ||^2
我们将SGD迭代插进去后可以得到，
L(θ_(t+1) )-L(θ_t )≤-α∇L(θ_t )^T ∇ ̂_(θ_t )+1/2 α^2 β|| ∇ ̂_(θ_t )  ||^2
使用A3并在第t+1次迭代中取期望可以得到（所有t次迭代相关的参数与t+1相互独立）
E[L(θ_(t+1) )|t+1]-L(θ_t )≤-α|| ∇L(θ_t) ||^2+1/2 α^2 β(Var[∇ ̂_(θ_t )│t+1]+|| E[∇ ̂_(θ_t )│t+1]  ||^2)
=-α(1-1/2 αβ)|| ∇L(θ_t) ||^2+1/2 α^2 βσ^2
≤-1/2 α|| ∇L(θ_t) ||^2+1/2 α^2 βσ^2
当我们对上面的等式再取一次期望时，条件期望的期望就变成了无条件期望。
得到：E[L(θ_(t+1) )]-E[L(θ_t )]≤-1/2 αE|| ∇L(θ_t) ||^2+1/2 α^2 βσ^2
当我们把上述不等式从t=1累加到t=T时候（经过T次迭代）就会得到：
L_(inf )-L(θ_1 )≤E[L(θ_T )]-E[L(θ_1 )]≤-1/2 α∑_(t=1)^T▒〖E|| ∇L(θ_t) ||^2 〗+1/2 Tα^2 βσ^2
对于等式的最左端和最右端经过移项并同除以αT处理即可得到T2
E〖||  ∇L_D (θ_t )  ||〗^2≤2(L(θ_1 )-L_inf )/αT+αβσ^2
T2证毕。

	T3准备：让G_H (∙)和G_θ (∙)作为G(∙)的一部分，分别对应∇_H 和∇_θ
定义G_θ^((l~m) ) (∇ ̂_(H^((m) ) ),C ̂^((m) ) )=G_θ^((l) ) (G_H^((l+1) ) (∙∙∙G_H^((m) ) (∇ ̂_(H^((m) ) ),C ̂^((m) ) )∙∙∙,C^((l+1) ) ),C^((l) ))，这里计算的是从∇ ̂_(H^((m) ) )处计算得到的∇ ̂_(θ^((l) ) )，这里仅仅在m层采用激活压缩精度，其他层均采用全精度。
Proposition 1：
Var [X] = E [Var [X | Y ]] + Var [E [X | Y ]] ,
G_θ^((l~m) ) (∇ ̂_(H^((m) ) ),C ̂^((m) ) )=G_θ^((l) ) (G_H^((l+1) ) (∙∙∙G_H^((m) ) (∇ ̂_(H^((m) ) ),C ̂^((m) ) )∙∙∙,C^((l+1) ) ),C^((l) ) ),    (*)
G_θ^((l~m) ) (∇ ̂_(H^((m) ) ) )=G_θ^((l) ) (G_H^((l+1) ) (∙∙∙G_H^((m) ) (∇ ̂_(H^((m) ) ),C^((m) ) )∙∙∙,C^((l+1) ) ),C^((l) ) ),        (**)
(*) m层C(~)压缩了，而(**)没有
T3定理描述（梯度方差）：Var[∇ ̂_(θ^((l) ) ) ]=Var[∇_(θ^((l) ) ) ]+∑_(m=l)^L▒E[Var[G_θ^((l~m) ) (∇ ̂_(H^((m) ) ),C ̂^((m) ) )│∇ ̂_(H^((m) ) ) ]] 
这里我们可以看到，采用激活压缩的梯度方差比全精度梯度方差多了一部分，如果多出来的那部分相较于全精度梯度方差非常小，那么就可以说明降低数值精度是可行的。并且多出来的部分方差是明确的，这就可以通过继续设计量化压缩策略来最小化这个值。
证明：
首先，因为∇_(H^((L)) )=∇ ̂_(H^((L)) )，按照定义有Var[G_θ^((l~L) ) (∇ ̂_(H^((L) ) ) )]=Var[G_θ^((l~L) ) (∇_(H^((L) ) ) )]=Var[∇_(θ^((l) ) )]  (13)
其次，对于所有的m<L，由∇ ̂_(H^((m)) )定义∇ ̂_(H^((m)) )=G_H^((m+1) ) (∇ ̂_(H^((m+1) ) ),C ̂^((m+1)))
得到,
Var[G_θ^((l~m) ) (∇ ̂_(H^((m) ) ) )]=Var[G_θ^((l~m) ) (G_H^((m+1) ) (∇ ̂_(H^((m+1) ) ),C ̂^((m+1))))]=Var[G_θ^((l~m+1) ) (∇ ̂_(H^((m+1) ) ),C ̂^((m+1) ) )]，
因为Proposition 1，因为是条件无偏估计，所以期望可以直接去掉(E[C ̂ ]=C,按照定义就省略了)，所以
 
Var[G_θ^((l~m) ) (∇ ̂_(H^((m) ) ) )]=Var[∇_(θ^((l) ) ) ]+∑_(j=m+1)^L▒〖E[Var[G_θ^((l~j) ) (∇ ̂_(H^((j) ) ),C ̂^((j) ))|∇ ̂_(H^((j)) )  ]]〗         (15)
注：对于等式(14)迭代至L求和即可得到等式(15)。

 
Var[∇ ̂_(θ^((l) ) ) ]=E[Var[G_θ^((l~l) ) (∇ ̂_(H^((l) ) ),C ̂^((l) ) )│∇ ̂_(H^((l) ) ) ]]+Var[∇_(θ^((l) ) ) ]+∑_(j=l+1)^L▒E[Var[G_θ^((l~j) ) (∇ ̂_(H^((j) ) ),C ̂^((j) ) )│∇ ̂_(H^((j) ) )  ]] 
=Var[∇_(θ^((l) ) ) ]+∑_(m=l)^L▒E[Var[G_θ^((l~m) ) (∇ ̂_(H^((m) ) ),C ̂^((m) ) )│∇ ̂_(H^((m) ) )  ]] 
T3证毕。

	压缩策略
分组量化：
 
注：这里Var[u ̂_ni ]=1/6需要证明下：
证：假设u ̂_ni∈[0,1]， 这里u ̂_ni 具体的取值无关紧要，因为算的是方差
因为有 w.prob.后面那个概率服从(0,1)的均匀分布，令这个值为p
翻译一下就是u ̂_ni 以p的概率取1，以1-p的概率取0，并且p服从U(0,1)均匀分布。
按照方差的定义：Var[u ̂_ni ]=E[u ̂_ni^2 ]-(E[u ̂_ni ])^2
u ̂_ni^2是一个新的变量，同样以p的概率取1，以1-p的概率取0，p服从(0,1)均匀分布（离散化的情况结论很明显）。
Var[u ̂_ni ]=E[E[u ̂_ni^2 ]-(E[u ̂_ni ])^2 ]=E[p-p^2 ]=E[p]-E[p^2 ]       (*)
E[p^2 ]=∫_0^1▒〖p^2*1 dp〗=p^3/3 |_0^1=1/3
∴(*)可化为 1/2-1/3=1/6
综上Var[u ̂_ni ]=1/6，证毕。

目标：最小化
 
首先考虑线性层：
 
 

SOME LAYERS的偏差和方差（特殊性）:
就是重复定理1-3，然后看期望相等，方差较小，灵敏度
卷积层：
定义：
 
梯度表示：
 
计算期望和方差：
 
 

归一化层，标准化层
向前传播公式（BatchNorm2d函数）：
 
反向传播公式：
 
对应之前讲到的C(~)这里是(X,m,s,w)，这里可以只讨论X，别的量级都是非常小的。
无偏性：
 
梯度方差：
 
 
Var[∇_X]∝∑_n▒(R_n^2)/(B_n^2 )
偏差（但是可以忽略）：
 
当N很大时候方差是可以忽视的。

激活层

池化层

Appendix：
	已知H^((l) )=H^((l-1) ) θ^((l))，那么∇_(H^((l-1)) )=∇_(H^((l)) ) θ^((l)^T ),∇_(θ^((l)) )=H^((l-1)^T ) ∇_(H^((l)) )
证明：这里H^((l) ),H^((l-1) ),θ^((l))其实为三个矩阵，为了方便表示，简写为C, A, B。
那么有C=AB
假设存在一个标量函数f，使得y=f(AB)，其中A维度为n×m，B维度为m×k
那么梯度的问题转化为要求∂y/∂A  和  ∂y/∂B
∵C=AB，那么有y=f(C)，同时y对于C中每个元素C_(i,j)的偏导数为∂y/(∂C_(i,j) )
同时，根据多元函数的链式法则有∂y/(∂A_(p,q) )=∑_(i,j)▒〖∂y/(∂C_(i,j) )*(∂C_(i,j))/(∂A_(p,q) )〗
又∵C_(i,j)=∑_h▒〖A_(i,h) B_(h,j) 〗
∴(∂C_(i,j))/(∂A_(p,q) )={█(B_(q,j)     i=p@0         i≠p)┤  (*)
将(*)带入∂y/(∂A_(p,q) )中有，∂y/(∂A_(p,q) )=∑_(i,j)▒〖∂y/(∂C_(i,j) )*(∂C_(i,j))/(∂A_(p,q) )〗=∑_j▒〖∂y/(∂C_(p,j) ) B_(q,j) 〗=∑_j▒〖∂y/(∂C_(p,j) ) B_(j,q)^T 〗
∴∂y/(∂A_(p,q) )=∂y/∂C B^T即有∇A=∇C*B^T
同理可得∇B=A^T ∇C
将H^((l) ),H^((l-1) ),θ^((l))回代即可得到∇_(H^((l-1)) )=∇_(H^((l)) ) θ^((l)^T ),∇_(θ^((l)) )=H^((l-1)^T ) ∇_(H^((l)) )
	条件期望的期望等于无条件的期望（总期望定理）
因为我们这里是离散化的情形，所以对离散化情况做出证明（连续函数同理，用积分）
 
	总方差定理Var [X] = E [Var [X | Y ]] + Var [E [X | Y ]]
 
